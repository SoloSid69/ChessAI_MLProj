import pandas as pd
import tensorflow as tf
import numpy as np

white_df = pd.read_csv('/content/WhiteDataset.csv')
black_df = pd.read_csv('/content/BlackDataset.csv')

column_names = ['board_array', 'rep_layer', 'move_rep', 'movelist', 'board_evaluation']

white_df = pd.read_csv('/content/WhiteDataset.csv', names=column_names)

black_df = pd.read_csv('/content/BlackDataset.csv', names=column_names)

combined_df = pd.concat([white_df, black_df], ignore_index=True)
combined_df.to_csv('/content/CombinedDataset.csv', index=False)

import tensorflow as tf

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation='relu')
        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation='linear')

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.conv2(x)
        return tf.keras.layers.add([inputs, x])

class AlphaZeroChessModel(tf.keras.Model):
    def __init__(self):
        super(AlphaZeroChessModel, self).__init__()
        # Define input layer separately
        self.input_layer = tf.keras.layers.InputLayer(input_shape=(8, 8, 17))

        # Convolutional layers
        self.conv1 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')
        self.res_blocks = [ResidualBlock(256, 3) for _ in range(19)]
        self.policy_conv = tf.keras.layers.Conv2D(2, 1, padding='same', activation='relu')
        self.policy_flatten = tf.keras.layers.Flatten()
        self.policy_output = tf.keras.layers.Dense(4096, activation='softmax')

        self.value_conv = tf.keras.layers.Conv2D(1, 1, padding='same', activation='relu')
        self.value_flatten = tf.keras.layers.Flatten()
        self.value_dense1 = tf.keras.layers.Dense(256, activation='relu')
        self.value_output = tf.keras.layers.Dense(1, activation='tanh')

    def call(self, inputs):
        x = self.input_layer(inputs)  # Pass input through input layer
        x = self.conv1(x)
        for block in self.res_blocks:
            x = block(x)
        policy = self.policy_output(self.policy_flatten(self.policy_conv(x)))
        value = self.value_output(self.value_dense1(self.value_flatten(self.value_conv(x))))
        return policy, value
    
    def forward(self, x):
        x_input = tf.identity(x)
        x = self.conv1(x)
        for block in self.res_blocks:
            x = block(x)
        x = self.policy_output(self.policy_flatten(self.policy_conv(x)))
        x_input = self.value_output(self.value_dense1(self.value_flatten(self.value_conv(x_input))))
        x = x + x_input
        return tf.keras.activations.relu(x)
    
    def train_step(self, data):
        inputs, targets = data
        with tf.GradientTape() as tape:
            policy, value = self(inputs, training=True)
            policy_loss = tf.keras.losses.CategoricalCrossentropy()(targets['policy'], policy)
            value_loss = tf.keras.losses.MeanSquaredError()(targets['value'], value)
            total_loss = policy_loss + value_loss
        gradients = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return {'total_loss': total_loss, 'policy_loss': policy_loss, 'value_loss': value_loss}

    def train(self, states, policies, values, batch_size=32, epochs=1):
        dataset = tf.data.Dataset.from_tensor_slices((states, {'policy': policies, 'value': values}))
        dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)
        self.compile(optimizer=tf.keras.optimizers.Adam())
        self.fit(dataset, epochs=epochs)

combined_df = pd.read_csv('/content/CombinedDataset.csv')

states = np.array(combined_df['board_array'].values.tolist())
policies = np.array(combined_df['move_rep'].values.tolist())
values = np.array(combined_df['board_evaluation'].values.tolist())

# Split data into training and validation sets
from sklearn.model_selection import train_test_split
states_train, states_val, policies_train, policies_val, values_train, values_val = train_test_split(
    states,
    policies,
    values,
    test_size=0.1,
    random_state=42)

# Instantiate Model
model = AlphaZeroChessModel()

# Compile Model
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss={'policy': 'categorical_crossentropy', 'value': 'mse'})

model.fit(x=states_train, y={'policy': policies_train, 'value': values_train},
          validation_data=(states_val, {'policy': policies_val, 'value': values_val}),
          batch_size=32, epochs=10)
